Traceback (most recent call last):
  File "main.py", line 1, in <module>
    from layer_stack import Transformer
  File "/home/jaehoon/Git/attention_is_allyouneed/jaehoon/layer_stack.py", line 1, in <module>
    from en_de_layer import Encoder, Decoder
  File "/home/jaehoon/Git/attention_is_allyouneed/jaehoon/en_de_layer.py", line 1, in <module>
    from model import *
  File "/home/jaehoon/Git/attention_is_allyouneed/jaehoon/model.py", line 1, in <module>
    from preprocess import Preprocess
  File "/home/jaehoon/Git/attention_is_allyouneed/jaehoon/preprocess.py", line 1, in <module>
    from sen_model_gen import Sentence_model_gen
  File "/home/jaehoon/Git/attention_is_allyouneed/jaehoon/sen_model_gen.py", line 1, in <module>
    import sentencepiece as spm
ModuleNotFoundError: No module named 'sentencepiece'
07/25/2019 11:51:28 AM: log file : ./save/log/201907251151-e9e3a99b.log 
07/25/2019 11:51:28 AM: Namespace(N_turns=6, batch_size=64, d_ff=2048, debug=False, dims=512, dropout=0.1, heads=8, input_file_dir='/home/jaehoon/Git/attention_is_allyouneed/dataset/de-en/train.en', learning_rate=0.0001, model_dir='/home/jaehoon/Git/attention_is_allyouneed/jaehoon/model/', n_epochs=200, output_file_dir='/home/jaehoon/Git/attention_is_allyouneed/dataset/de-en/train.de', print_fre=100, save_log=True, sentence_model_gen='False', vocab_size=5000) 
07/25/2019 11:51:30 AM: ----- tokenize en first data ----- 
07/25/2019 11:51:30 AM: ['▁David', '▁G', 'al', 'lo', ':', '▁This', '▁is', '▁Bill', '▁La', 'n', 'ge', '.', '▁I', "'", 'm', '▁Da', 've', '▁G', 'al', 'lo', '.'] 
07/25/2019 11:51:30 AM: [2394, 378, 103, 342, 65, 111, 20, 2742, 874, 78, 419, 8, 17, 12, 45, 1485, 68, 378, 103, 342, 8] 
07/25/2019 11:51:30 AM: {} 
07/25/2019 11:51:43 AM: ----- tokenize de first data ----- 
07/25/2019 11:51:43 AM: ['▁David', '▁Gal', 'lo', ':', '▁Das', '▁ist', '▁Bill', '▁Lang', 'e', '.', '▁Ich', '▁bin', '▁Da', 've', '▁Gal', 'lo', '.'] 
07/25/2019 11:51:43 AM: [2945, 3275, 376, 43, 54, 17, 2918, 2984, 12, 8, 40, 192, 204, 911, 3275, 376, 8] 
07/25/2019 11:51:43 AM: {} 
07/25/2019 11:51:56 AM: max length of en word : 16 
07/25/2019 11:51:56 AM: max length of de word : 16 
07/25/2019 11:51:56 AM: max length of en sentence : 846 
07/25/2019 11:51:56 AM: max length of de sentence : 919 
07/25/2019 11:51:57 AM: max sentence length cutoff value 
07/25/2019 11:51:57 AM: 31 
07/25/2019 11:51:57 AM: max sentence length cutoff value 
07/25/2019 11:51:57 AM: 34 
07/25/2019 11:51:57 AM: inputs / outputs shape, first : inputs / second : outputs 
07/25/2019 11:51:57 AM: (148988, 34) 
07/25/2019 11:51:57 AM: (148988, 34) 
main.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  label = torch.tensor(torch.reshape(y, (-1,)), dtype=torch.int64)
/home/jaehoon/Git/attention_is_allyouneed/jaehoon/model.py:103: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  scores = sm_model(scores)
/home/jaehoon/Git/attention_is_allyouneed/jaehoon/layer_stack.py:64: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  output = self.softmax(self.out(d_outputs))
07/25/2019 11:52:12 AM: time = 0m, epoch 1, iter = 100, loss = 8.515, 0s per 100 iters 
07/25/2019 11:52:26 AM: time = 0m, epoch 1, iter = 200, loss = 8.510, 0s per 100 iters 
07/25/2019 11:52:40 AM: time = 1m, epoch 1, iter = 300, loss = 8.508, 0s per 100 iters 
07/25/2019 11:52:54 AM: time = 1m, epoch 1, iter = 400, loss = 8.506, 0s per 100 iters 
07/25/2019 11:53:09 AM: time = 1m, epoch 1, iter = 500, loss = 8.503, 0s per 100 iters 
07/25/2019 11:53:23 AM: time = 1m, epoch 1, iter = 600, loss = 8.502, 0s per 100 iters 
07/25/2019 11:53:37 AM: time = 2m, epoch 1, iter = 700, loss = 8.501, 0s per 100 iters 
07/25/2019 11:53:51 AM: time = 2m, epoch 1, iter = 800, loss = 8.500, 0s per 100 iters 
07/25/2019 11:54:05 AM: time = 2m, epoch 1, iter = 900, loss = 8.499, 0s per 100 iters 
07/25/2019 11:54:20 AM: time = 2m, epoch 1, iter = 1000, loss = 8.500, 0s per 100 iters 
07/25/2019 11:54:34 AM: time = 3m, epoch 1, iter = 1100, loss = 8.499, 0s per 100 iters 
07/25/2019 11:54:48 AM: time = 3m, epoch 1, iter = 1200, loss = 8.498, 0s per 100 iters 
07/25/2019 11:55:02 AM: time = 3m, epoch 1, iter = 1300, loss = 8.498, 0s per 100 iters 
07/25/2019 11:55:16 AM: time = 3m, epoch 1, iter = 1400, loss = 8.499, 0s per 100 iters 
07/25/2019 11:55:30 AM: time = 3m, epoch 1, iter = 1500, loss = 8.498, 0s per 100 iters 
07/25/2019 11:55:44 AM: time = 4m, epoch 1, iter = 1600, loss = 8.498, 0s per 100 iters 
07/25/2019 11:55:59 AM: time = 4m, epoch 1, iter = 1700, loss = 8.498, 0s per 100 iters 
07/25/2019 11:56:13 AM: time = 4m, epoch 1, iter = 1800, loss = 8.498, 0s per 100 iters 
07/25/2019 11:56:27 AM: time = 4m, epoch 1, iter = 1900, loss = 8.496, 0s per 100 iters 
07/25/2019 11:56:41 AM: time = 5m, epoch 1, iter = 2000, loss = 8.496, 0s per 100 iters 
07/25/2019 11:56:55 AM: time = 5m, epoch 1, iter = 2100, loss = 8.498, 0s per 100 iters 
07/25/2019 11:57:09 AM: time = 5m, epoch 1, iter = 2200, loss = 8.497, 0s per 100 iters 
07/25/2019 11:57:23 AM: time = 5m, epoch 1, iter = 2300, loss = 8.497, 0s per 100 iters 
HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))
time = 0m, epoch 1, iter = 100, loss = 8.515, 42s per 100 iters
time = 0m, epoch 1, iter = 200, loss = 8.510, 14s per 100 iters
time = 1m, epoch 1, iter = 300, loss = 8.508, 14s per 100 iters
time = 1m, epoch 1, iter = 400, loss = 8.506, 14s per 100 iters
time = 1m, epoch 1, iter = 500, loss = 8.503, 14s per 100 iters
time = 1m, epoch 1, iter = 600, loss = 8.502, 14s per 100 iters
time = 2m, epoch 1, iter = 700, loss = 8.501, 14s per 100 iters
time = 2m, epoch 1, iter = 800, loss = 8.500, 14s per 100 iters
time = 2m, epoch 1, iter = 900, loss = 8.499, 14s per 100 iters
time = 2m, epoch 1, iter = 1000, loss = 8.500, 14s per 100 iters
time = 3m, epoch 1, iter = 1100, loss = 8.499, 14s per 100 iters
time = 3m, epoch 1, iter = 1200, loss = 8.498, 14s per 100 iters
time = 3m, epoch 1, iter = 1300, loss = 8.498, 14s per 100 iters
time = 3m, epoch 1, iter = 1400, loss = 8.499, 14s per 100 iters
time = 3m, epoch 1, iter = 1500, loss = 8.498, 14s per 100 iters
time = 4m, epoch 1, iter = 1600, loss = 8.498, 14s per 100 iters
time = 4m, epoch 1, iter = 1700, loss = 8.498, 14s per 100 iters
time = 4m, epoch 1, iter = 1800, loss = 8.498, 14s per 100 iters
time = 4m, epoch 1, iter = 1900, loss = 8.496, 14s per 100 iters
time = 5m, epoch 1, iter = 2000, loss = 8.496, 14s per 100 iters
time = 5m, epoch 1, iter = 2100, loss = 8.498, 14s per 100 iters
time = 5m, epoch 1, iter = 2200, loss = 8.497, 14s per 100 iters
time = 5m, epoch 1, iter = 2300, loss = 8.497, 14s per 100 iters

training is finished 
